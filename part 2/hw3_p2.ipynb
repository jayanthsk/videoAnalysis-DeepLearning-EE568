{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMcOocx65GvL"
      },
      "source": [
        "# Detectron2 Beginner's Tutorial\n",
        "\n",
        "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
        "\n",
        "Welcome to detectron2! This is the official colab tutorial of detectron2. Here, we will go through some basics usage of detectron2, including the following:\n",
        "* Run inference on images or videos, with an existing detectron2 model\n",
        "* Train a detectron2 model on a new dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s85Bs_9C5GvM"
      },
      "outputs": [],
      "source": [
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRPwrL9M5GvM"
      },
      "outputs": [],
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eKBD7tj5GvN"
      },
      "outputs": [],
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buDHeqsY5GvO"
      },
      "source": [
        "### Run a pretrained Detectron2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI0lB5OV5GvO"
      },
      "source": [
        "We first download some image from the given URLs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0ZU8y2l5GvO"
      },
      "outputs": [],
      "source": [
        "!wget http://images.cocodataset.org/val2017/000000007574.jpg -q -O input.jpg\n",
        "im_input = cv2.imread(\"./input.jpg\")\n",
        "cv2_imshow(im_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aa_cO0J5GvO"
      },
      "outputs": [],
      "source": [
        "!wget http://images.cocodataset.org/val2017/000000013923.jpg -q -O test1.jpg\n",
        "im_test1 = cv2.imread(\"./test1.jpg\")\n",
        "cv2_imshow(im_test1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3ZpSTs65GvP"
      },
      "outputs": [],
      "source": [
        "!wget http://images.cocodataset.org/val2017/000000018380.jpg -q -O test2.jpg\n",
        "im_test2 = cv2.imread(\"./test2.jpg\")\n",
        "cv2_imshow(im_test2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUJeCMzE5GvP"
      },
      "source": [
        "We can see there are multiple objects in these images: bottles, tables, chairs, people, etc. Let us see if we can detect them all by using a pre-trained model given by Detectron2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2AlFrkg5GvP"
      },
      "source": [
        "Let's take a look at the model output.\n",
        "\n",
        "In inference mode, the builtin model outputs a `list[dict]`, one dict for each image. For the object detection task, the dict contain the following fields:\n",
        "\n",
        "*   \"instances\": Instances object with the following fields:\n",
        "    * \"pred_boxes\": Storing N boxes, one for each detected instance.\n",
        "    * \"scores\": a vector of N scores.\n",
        "    * \"pred_classes\": a vector of N labels in range [0, num_categories].\n",
        "\n",
        "For more details, please see https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro0tHAIh5ih7"
      },
      "outputs": [],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST= 0.5  # set threshold for this model\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "outputs = predictor(im_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQYI8Evr5GvQ"
      },
      "outputs": [],
      "source": [
        "print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA8TKlbO5GvQ"
      },
      "outputs": [],
      "source": [
        "print(outputs[\"instances\"].pred_classes)\n",
        "print(outputs[\"instances\"].pred_boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JjlBw305GvQ"
      },
      "outputs": [],
      "source": [
        "outputs_q1q2 = {'q1': [], 'q2': []}\n",
        "outputs_q1q2['q1'].append(outputs[\"instances\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slLDE6it5GvQ"
      },
      "outputs": [],
      "source": [
        "# We can use \"Visualizer\" to draw the predictions on the image\n",
        "v = Visualizer(im_input[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqX7KEVn5GvQ"
      },
      "source": [
        "AWESOME!!! Great progress so far! We are able to detect sink, microwave, bottle and even refrigerator! At this point, we have used the pre-trained model to do the inference on the given image. There are in total 17 objects are being detected. The image is adopted from the [MS-COCO](https://cocodataset.org/#home) dataset and there are 81 classes including person, bicycle, car, etc. You may find the id-category mapping [here](https://gist.github.com/AruniRC/7b3dadd004da04c80198557db5da4bda)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqdjc7zH5GvQ"
      },
      "source": [
        "The model we just used is `COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml`. Actually, the Detectron2 provides us more than that, you may find great amouts of models for different tasks in the given [MODEL_ZOO](https://github.com/facebookresearch/detectron2/tree/master/configs). What about we try a different model to see what its output will look like?\n",
        "\n",
        "\n",
        "* Q1 (5%): Object Detection. Use the same configuration `COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml`, with IoU threshold of 0.5 (`SCORE_THRESH_TEST=0.5`), to also run inference on the rest two images (test1.jpg & test2.jpg) and view the outputs with bounding boxes.\n",
        "\n",
        "* Q2: Object Detection. Use the `COCO-Detection/faster_rcnn_R_101_FPN_3X.yaml`, which has a ResNet-101 as the backbone, with IoU threshold of 0.5 and view the outputs of all three images with bounding boxes. By looking at the outputs, can you find the difference with the one `COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml` we used in Q1? (e.g., numbers of objects, confidence scores, ...)\n",
        "\n",
        "* Q3: Object Detection. Use the `COCO-Detection/faster_rcnn_R_101_FPN_3X.yaml` with an IoU threshold of 0.9 and view the outputs of all three images with bounding boxes.\n",
        "\n",
        "* Q4 (5%): Instance Segmentation. The models we have tried in Q1-Q3 are the Faster R-CNN models for object detection. Here, letâ€™s try a Mask R-CNN model `COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml`, with IoU threshold of 0.5, to perform the instance segmentation and view the outputs of all three images with segmentation masks. Compare the difference of outputs between an object detection model with an instance segmentation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvntnCGa5GvR"
      },
      "outputs": [],
      "source": [
        "# todo: Q1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vDEupn55GvR"
      },
      "outputs": [],
      "source": [
        "# todo: Q2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhQpOiLD5GvR"
      },
      "outputs": [],
      "source": [
        "# todo: Q3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tavx_7iU5GvR"
      },
      "outputs": [],
      "source": [
        "# todo: Q4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LxhMlNu5GvR"
      },
      "source": [
        "# Now let's train on the provided sportsmot dataset\n",
        "please upload the provided sportsmot dataset first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZ9n8Orz5GvS"
      },
      "outputs": [],
      "source": [
        "!unzip sportsmot.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmgXJRCW5GvS"
      },
      "outputs": [],
      "source": [
        "from detectron2.structures import BoxMode\n",
        "\n",
        "def get_dataset_dicts(data_root, txt_file):\n",
        "    dataset_dicts = []\n",
        "    filenames = []\n",
        "    csv_path = os.path.join(data_root, txt_file)\n",
        "    with open(csv_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            filenames.append(line.rstrip())\n",
        "\n",
        "    for idx, filename in enumerate(filenames):\n",
        "        record = {}\n",
        "\n",
        "        image_path = os.path.join(data_root, filename)\n",
        "\n",
        "        im = cv2.imread(image_path)\n",
        "        if im is None:\n",
        "            continue\n",
        "        height, width = im.shape[:2]\n",
        "\n",
        "        record['file_name'] = image_path\n",
        "        record['image_id'] = idx\n",
        "        record['height'] = height\n",
        "        record['width'] = width\n",
        "\n",
        "        image_filename = os.path.basename(filename)\n",
        "        image_name = os.path.splitext(image_filename)[0]\n",
        "        annotation_path = os.path.join(data_root, 'labels', '{}.txt'.format(image_name))\n",
        "        annotation_rows = []\n",
        "\n",
        "        with open(annotation_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                temp = line.rstrip().split(\" \")\n",
        "                annotation_rows.append(temp)\n",
        "\n",
        "        objs = []\n",
        "        for row in annotation_rows:\n",
        "            xcentre = int(float(row[1])*width)\n",
        "            ycentre = int(float(row[2])*height)\n",
        "            bwidth = int(float(row[3])*width)\n",
        "            bheight = int(float(row[4])*height)\n",
        "\n",
        "            xmin = int(xcentre - bwidth/2)\n",
        "            ymin = int(ycentre - bheight/2)\n",
        "            xmax = xmin  + bwidth\n",
        "            ymax = ymin + bheight\n",
        "\n",
        "            obj= {\n",
        "                'bbox': [xmin, ymin, xmax, ymax],\n",
        "                'bbox_mode': BoxMode.XYXY_ABS,\n",
        "                # alternatively, we can use bbox_mode = BoxMode.XYWH_ABS\n",
        "                # 'bbox': [xmin, ymin, bwidth, bheight],\n",
        "                # 'bbox_mode': BoxMode.XYWH_ABS,\n",
        "                'category_id': int(row[0]),\n",
        "                'iscrowd': 0\n",
        "            }\n",
        "\n",
        "            objs.append(obj)\n",
        "        record['annotations'] = objs\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HkL-XAf5GvS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "# Metadata configurations\n",
        "data_root = \"sportsmot\"\n",
        "train_txt = \"sportsmot_train.txt\"\n",
        "test_txt = \"sportsmot_test.txt\"\n",
        "\n",
        "train_data_name = \"train\"\n",
        "test_data_name = \"test\"\n",
        "\n",
        "thing_classes = [\"person\"]\n",
        "\n",
        "output_dir = \"./outputs\"\n",
        "\n",
        "def count_lines(fname):\n",
        "    with open(fname) as f:\n",
        "        for i, l in enumerate(f):\n",
        "            pass\n",
        "    return i + 1\n",
        "\n",
        "train_img_count = count_lines(os.path.join(data_root, train_txt))\n",
        "print(\"There are {} samples in training data\".format(train_img_count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-Vmcx9b5GvS"
      },
      "outputs": [],
      "source": [
        "# Register the traffic_sign_train datasets\n",
        "DatasetCatalog.register(name=train_data_name,\n",
        "                        func=lambda: get_dataset_dicts(data_root, train_txt))\n",
        "train_metadata = MetadataCatalog.get(train_data_name).set(thing_classes=thing_classes)\n",
        "\n",
        "# Register the traffic_sign_test datasets\n",
        "DatasetCatalog.register(name=test_data_name,\n",
        "                        func=lambda: get_dataset_dicts(data_root, test_txt))\n",
        "test_metadata = MetadataCatalog.get(test_data_name).set(thing_classes=thing_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8S_fFnr5GvS"
      },
      "outputs": [],
      "source": [
        "train_data_dict = get_dataset_dicts(data_root, train_txt)\n",
        "\n",
        "for d in random.sample(train_data_dict, 3):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=train_metadata, scale=0.5)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80rEfivD5GvT"
      },
      "outputs": [],
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (train_data_name,)\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\") # let's trainining initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.0001  # pick a good LR\n",
        "cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(thing_classes)  # only has one class (traffic-sign)\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
        "cfg.OUTPUT_DIR = output_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm52Zi2a5GvT"
      },
      "outputs": [],
      "source": [
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSYtNy6w5GvT"
      },
      "outputs": [],
      "source": [
        "# cfg alrady contains everything we've set previously. Now we changed it a little bit for inference:\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhoVf2Sn5GvT"
      },
      "outputs": [],
      "source": [
        "from detectron2.utils.visualizer import ColorMode\n",
        "\n",
        "test_data_dict = get_dataset_dicts(data_root, test_txt)\n",
        "\n",
        "for d in random.sample(test_data_dict, 3):\n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=test_metadata,\n",
        "                   scale=0.5,\n",
        "                   )\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdain9QM5GvT"
      },
      "source": [
        "## Problem 2\n",
        "Multi-Object Tracking  \n",
        "\n",
        "After training the detector, now we want to implement tracking on the testing video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6D6yiAY5GvT"
      },
      "outputs": [],
      "source": [
        "# Let's start with a detector class\n",
        "class detector:\n",
        "    def __init__(self,predictor):\n",
        "        self.model = predictor\n",
        "\n",
        "    def predict(self,img):\n",
        "        pred = self.model(img)\n",
        "        pred = [pred['instances'][i].pred_boxes.tensor.tolist()[0] for i in range(len(pred['instances']))]\n",
        "        return pred\n",
        "\n",
        "\n",
        "# TODO \n",
        "# Initiate a detector and inference on the first test image('sportsmot/JPEGImages/test_000001.jpg') and print the bounding box prediction.\n",
        "# The output format should be x1,y1,x2,y2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3y1GUde5GvT"
      },
      "source": [
        "Now you will implement your own tracker!  \n",
        "\n",
        "Let's start with the IoU function and tracklet class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRhcmyqH5GvU"
      },
      "outputs": [],
      "source": [
        "# calculate the overlap ratio of two bounding boxes\n",
        "def calculate_iou(bbox1, bbox2):\n",
        "\n",
        "    x1_1, y1_1, x2_1, y2_1 = bbox1\n",
        "    x1_2, y1_2, x2_2, y2_2 = bbox2\n",
        "    x_left = max(x1_1, x1_2)\n",
        "    y_top = max(y1_1, y1_2)\n",
        "    x_right = min(x2_1, x2_2)\n",
        "    y_bottom = min(y2_1, y2_2)\n",
        "\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return 0.0\n",
        "\n",
        "    area_bbox1 = (x2_1 - x1_1 + 1) * (y2_1 - y1_1 + 1)\n",
        "    area_bbox2 = (x2_2 - x1_2 + 1) * (y2_2 - y1_2 + 1)\n",
        "    intersection_area = (x_right - x_left + 1) * (y_bottom - y_top + 1)\n",
        "\n",
        "    iou = intersection_area / float(area_bbox1 + area_bbox2 - intersection_area)\n",
        "\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j59OB0--5GvU"
      },
      "outputs": [],
      "source": [
        "# base class for tracklet, you can of course add more features and try to improve the performance!\n",
        "class tracklet:\n",
        "    def __init__(self,tracking_ID,box):\n",
        "        self.ID = tracking_ID\n",
        "        self.cur_box = box\n",
        "        self.alive = True\n",
        "\n",
        "    def update(self,box):\n",
        "        self.cur_box = box\n",
        "\n",
        "    def close(self):\n",
        "        self.alive = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vciZ3VWe5GvU"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "class IoU_Tracker:\n",
        "    def __init__(self):\n",
        "        self.all_tracklets = [] # this saves all the tracklets so that we can know how many tracklets we have\n",
        "        self.cur_tracklets = [] # this saves tracklets from the last frame for current frame's association\n",
        "        self.online_tracklets = [] # this saves the tracklets after association, so we can pass the tracking result to output\n",
        "\n",
        "    def update(self,frame_id,detection):\n",
        "\n",
        "        if frame_id%100 == 0:\n",
        "            print(f'Running tracking || current frame {frame_id}')\n",
        "\n",
        "        if len(self.cur_tracklets) == 0:\n",
        "            for det in detection:\n",
        "                new_tracklet = tracklet(len(self.all_tracklets)+1,det)\n",
        "                self.cur_tracklets.append(new_tracklet)\n",
        "                self.all_tracklets.append(new_tracklet)\n",
        "        else:\n",
        "            cost_matrix = np.zeros((len(self.cur_tracklets),len(detection)))\n",
        "\n",
        "            # build up cost matrix, each element in cost matrix should be 1-IoU between tracklet and detection\n",
        "            for row in range(len(self.cur_tracklets)):\n",
        "                for col in range(len(detection)):\n",
        "                    cost_matrix[row][col] = 1 - calculate_iou(self.cur_tracklets[row].cur_box,detection[col])\n",
        "\n",
        "            row_inds,col_inds = linear_sum_assignment(cost_matrix)\n",
        "\n",
        "            matches = min(len(row_inds),len(col_inds))\n",
        "\n",
        "            for idx,trk in enumerate(self.cur_tracklets):\n",
        "                if idx not in row_inds: # if it is not matched in the above Hungarian algorithm stage\n",
        "                # TODO\n",
        "                # use tracklet's close function to kill those unmatched tracklets\n",
        "\n",
        "            for idx,det in enumerate(detection):\n",
        "                if idx not in col_inds: # if it is not matched in the above Hungarian algorithm stage\n",
        "                # TODO\n",
        "                # initiate unmatched detections as new tracklets\n",
        "\n",
        "\n",
        "            for idx in range(matches):\n",
        "                row,col = row_inds[idx],col_inds[idx]\n",
        "                if cost_matrix[row][col] == 1:\n",
        "                    # TODO 1. Kill the tracklet using tracklet's close function\n",
        "                    # TODO 2. Initiate a new tracklet for the new detection\n",
        "                    # TODO 3. Append new tracklet to the current tracklets and all tracklets\n",
        "                else:\n",
        "                    self.cur_tracklets[row].update(detection[col])\n",
        "\n",
        "        self.cur_tracklets = [trk for trk in self.cur_tracklets if trk.alive]\n",
        "\n",
        "        return self.cur_tracklets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z08SB_lN5GvU"
      },
      "source": [
        "Now it's time to run tracking!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11Xk2jEI5GvU"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "# TODO\n",
        "# initialize your detector (the one you just trained)\n",
        "\n",
        "# TODO\n",
        "# initialize tracker\n",
        "\n",
        "# TODO run tracking\n",
        "images = np.loadtxt('sportsmot/sportsmot_test.txt',dtype=str)\n",
        "results = []\n",
        "\n",
        "print(f'length of sequence is {len(images)}')\n",
        "\n",
        "for frame_id, img_path in enumerate(images,1):\n",
        "    img = cv2.imread('sportsmot/'+img_path)\n",
        "    # TODO get detection with your model\n",
        "    detection = \n",
        "\n",
        "    # TODO update tracker with detection\n",
        "    result = \n",
        "\n",
        "    for track in result:\n",
        "        x1,y1,x2,y2 = track.cur_box\n",
        "        track_id = track.ID\n",
        "        results.append(f'{int(frame_id)},{int(track_id)},{int(x1)},{int(y1)},{int(x2-x1)},{int(y2-y1)},{1},{1},{1}') # format: frame_id, track_id, x, y, w, h, score, x_coord, y_coord\n",
        "\n",
        "with open('results.txt','w') as f:\n",
        "    for line in results:\n",
        "        f.writelines(line)\n",
        "        f.writelines('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLTEyCJE5GvV"
      },
      "source": [
        "Some packages are required to do the evaluation and visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wNLo16y5GvV"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install motmetrics\n",
        "!pip install pytrec_eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfsmzxw_5GvV"
      },
      "source": [
        "Now you can evaluate the tracking results using the standard MOT CLEAR Metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_M77s6K5GvV"
      },
      "outputs": [],
      "source": [
        "# TODO Evaluate your tracking performance\n",
        "!python eval.py gt.txt results.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcPAsIPD_bKM"
      },
      "outputs": [],
      "source": [
        "# TODO To get the visualization result\n",
        "!python vis.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9ztXz_C5GvV"
      },
      "source": [
        "Can you try to improve the performance with differnet tracking tricks? Here, we provide several potential tricks to increase the tracking performance, including:\n",
        "1. Increase detection performance using a lower confidence threshold in detector - some low score detections that is filtered might be false negative detections! (or switch to a higher threshold if you have too many false positive)\n",
        "2. Extend the tracklet's age and allow tracklets to live for more than 1 frame.\n",
        "3. Incorporate different association method into tracking process, perhaps using differnt types of IoU or even use bounding box distance.\n",
        "\n",
        "Please include your experiment results in your report. We give grade not only based on performance, but also your effort and finding."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
